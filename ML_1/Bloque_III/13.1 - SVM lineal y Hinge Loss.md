```insta-toc
---
title:
  name: ""
  level: 1
  center: false
exclude: ""
style:
  listType: dash
omit: []
levels:
  min: 1
  max: 6
---

# 

- Máquina de vectores soporte lineales y Hinge Loss
    - ¿Con qué modelo lineal nos  quedaríamos?
    - Pasos previos
    - Ecuaciones de la superficie de decisión
    - Distancia del hiperplano al origen
    - Márgenes
        - ¡¿Cómo elegimos los parámetros del modelo?!
    - Márgenes rígidos
    - Márgenes laxos
- Riesgo estructural
- Pérdida de Hinge
- Ejemplo de pérdida hinge con descenso de gradientes:
```
![[Captura de pantalla 2025-03-19 173341.png]]
## Máquina de vectores soporte lineales y Hinge Loss
El objetivo de los funciones de pérdida que hemos visto hasta ahora (MSE  y MAE) tenían como objetivo minimizar el riesgo empírico (basado en datos, en los ejemplos del conjunto de entrenamiento solamente)

SVM es un modelo que tiene como objetivo minimizar el riesgo estructural.

### ¿Con qué modelo lineal nos  quedaríamos?
![[Pasted image 20250312173620.png]]
La mejor es la C4 porque es la que tiene en el margen más grande.
SVM $\rightarrow$  va a intentar maximizar el margen del modelo, a parte de que sea el mejor modelo.


### Pasos previos
- SVM es un modelo para la clasificación binaria, donde las dos clases posibles son + y -
- Para modelos modelos lineales esto significa que la superficie decisión es un hiperplano y los elementos de su vector característico son los parámetros que hemos obtenido a partir del aprendizaje de los datos
- Lo vamos a explicar con dos características ($x_1$ y $x_2$) para poder dibujarlo. Aunque todas las explicaciones se puede generalizar a $D$ dimensiones.

### Ecuaciones de la superficie de decisión

- El plano separador son todos aquellos puntos que satisfacen la ecuación: $$w_0 +  w^T x = 0,$$

En concreto, para nuestra explicación quedaría:  
$$

w_0 + w_1x_1 + w_2x_2 = 0.

$$

- ¿Cuál es la ecuación para dos dimensiones?
$$
w_0 + w_1x_1 + w_2x_2 = 0
$$
- Por otro lado, podemos definir el plano separador sobre el plano $\overline{X_1X_2}$ a partir de un punto $(0,b)$ y un vector director $(v_1, v_2)$ con la ecuación

$$

x_2 = \left(\frac{v_2}{v_1}\right)x_1 + b

$$
- Por último, recordamos que el vector característico del plano separador son los coeficientes $(w_1, w_2).$
- De las dos expresiones se deduce que:

$$

\left(-b\right) + \left(-\frac{v_2}{v_1}\right)x_1 + \left(1\right)x_2 = 0,

\quad\text{ pero también que}\quad

\left(b\right) + \left(\frac{v_2}{v_1}\right)x_1 + \left(-1\right)x_2 = 0.

$$

- De los dos, elegimos el de la izquierda.
- Todo ello se puede ver en el siguiente código, que:
- Dibuja una recta en el plano $\overline{X_1X_2}$ definida por un vector director y con un punto de intercepción elegido por nosotros.
![[Pasted image 20250324001609.png|275]]

- Asignaremos la clase $+$ a un ejemplo  $\bf x$ cuando: $f({\bf x},{\bf w}) = w_0 + w_1x_1 + w_2x_2 > 0$.
- Entonces, de las dos posibilidades, elegimos aquella que deje la clase $+$ "encima" del hiperplano clasificador. 
- Esto ocurre en la que estaba más a la izquierda, o sea

$$
w_0 = -b \quad,\quad  w_1 = -\frac{v_2}{v_1} \quad,\quad  w_2 = 1
$$
![[Pasted image 20250324002212.png|275]]
### Distancia del hiperplano al origen 
- La distancia del plano separador al origen se calcula geométricamente siguiendo los pasos de la siguiente figura: 
![[Pasted image 20250324002452.png]]
- Por construcción se deduce que la distancia $~d = b \sin (\gamma)$.
- Para obtener el $\sin(\gamma)$ usamos el vector característico. 
  - El vector característico se descompone en su componente horizontal y vertical.
  - Sabemos que la vertical $~w_2 = 1 = \Vert {\bf w} \Vert \sin (\gamma)$.
	- Por tanto $~ d = \frac{b}{\Vert {\bf w} \Vert}$
### Márgenes
- En SVM el margen es la zona interior delimitada por dos superficies paralelas a la superficie de decisión, una por encima y otra por debajo

- Estamos asumiendo el modelo lineal $w_0 + w_1x_1 + w_2x_2$, cuya superficie de separación pasa por el punto $~(0,b).$
- Creamos 2 planos paralelos:
	- uno por encima, que pase por el punto $~(0, b+1),~$cuya  distancia al origen es $d^{(+)} = \frac{b+1}{\Vert {\bf w} \Vert}$.
	- otro por debajo, que pase por $~(0, b-1),~$ cuya distancia al origen es $d^{(-)} = \frac{b-1}{\Vert {\bf w} \Vert}$.
- El **margen** $\varepsilon~$ es la distancia que hay entre los dos planos paralelos, es decir:

$$

 \varepsilon = d^{(+)} - d^{(-)} = \frac{b+1 -(b-1)}{\Vert {\bf w}\Vert} = \frac{2}{\Vert{\bf w}\Vert}

$$


$$\fbox{El margen $\varepsilon$ CRECE cuando la norma $\Vert\bf w \Vert$ DECRECE}$$

- Por tanto, para **maximizar** $\varepsilon$ tenemos que **minimizar** $\Vert\bf w\Vert$.
- Y esto implica que el margen depende del modelo, puesto que $~{\bf w}~$ son sus parámetros.
- En otras palabras, modificando los parámetros del modelo aumentamos o disminuimos el margen.

#### ¡¿Cómo elegimos los parámetros del modelo?!
- Lo único que sabemos es que queremos minimizar la norma $\Vert\bf w\Vert$.
- Pero eso lo podemos lograr sin necesidad de ejemplos de entrenamiento, es tan fácil como hacer $w_0 = w_1 = w_2 = \cdots = w_D = 0~$ (la "solución trivial")

- Los **ejemplos de entrenamiento** se utilizan para añadir **restricciones**.

- Como queremos construir un clasificador, la restricción natural es que clasifique correctamente los ejemplos de entrenamiento; es decir:
	- Si el ejemplo $\bf x$ es de la clase $+$ entonces $f({\bf x};{\bf w}) > 0$
	- Si el ejemplo $\bf x$ es de la clase $-$ entonces $f({\bf x};{\bf w}) < 0$

Sea $~y = \{+1,-1\},~$ la etiqueta del ejemplo $\bf x$, entonces las dos restricciones se formalizan como:
$$

(y)\cdot(w_0 + w_1x_1 + w_2x_2) > 0,

$$
es decir:
- cuando $y=+1$ el ejemplo evaluado en el plano separador debe quedar "encima", y por tanto tenemos la multiplicación de dos números positivos.
- cuando $y=-1$ el ejemplo evaluado en el plano separador debe quedar "debajo", y por tanto tenemos la multiplicación de dos números negativos.
### Márgenes rígidos

- El efecto de la restricción simplemente es encontrar el plano que separa en dos el conjunto de datos impidiendo que haya ejemplos de entrenamiento mal clasificados.

- PERO **NO** hemos impuesto ninguna "zona de exclusión" donde no debe haber ejemplos.

- Para crear esta "zona de exclusión" modificamos la restricción del siguiente modo:
$$
(y)\cdot(w_0 + w_1x_1 + w_2x_2) > 1.
$$
	- Ponemos $>1$ para imponer un "margen" normalizado.
- cuando $y=+1$ el ejemplo evaluado en el plano separador debe quedar "encima" , con un valor mayor que $+1$.
- cuando $y=-1$ el ejemplo evaluado en el plano separador debe quedar "debajo", con un valor menor que $-1$.
```
MODELO:
w=
 [[-2.]
 [ 2.]
 [ 1.]]

DATOS:
datos=
     x1   x2  y
0  0.8  1.5  1
1  0.9  0.6  1
2  0.5  0.2 -1
3  0.1  0.2 -1

-Ej.0: y = [1], f(x,w)=[1.1] -> cumple restricción: True

-Ej.1: y = [1], f(x,w)=[0.4] -> cumple restricción: False

-Ej.2: y = [-1], f(x,w)=[-0.8] -> cumple restricción: False

-Ej.3: y = [-1], f(x,w)=[-1.6] -> cumple restricción: True
```
### Márgenes laxos

- Con márgenes rígidos (_hard_) sólo es posible encontrar solución al problema si el conjunto de datos se puede separar con un clasificador lineal.

- Un solo ejemplo que no cumpla la restricción hace que no haya solución.

- Para resolverlo se utilizan márgenes laxos (_soft_), que permiten ejemplos en la "zona de exclusión" PERO **penalizándolos**

- En la imagen de abajo se muestra la diferencia entre ambos.
 - Los ejemplos marcados con un circulo son ejemplos que están penalizados, pero a cambio el margen ha aumentado mucho y el plano clasificador es diferente.
![[Pasted image 20250324003722.png]]

- Los márgenes laxos introducen una variable _**slack**_ $~\zeta^{(i)}>0~$ para cada ejemplo, $~i=1,2,\ldots,N;$ tal que:
	- $\zeta^{(i)} = 0~$ si el ejemplo $~{\bf x}^{(i)}~$ está correctamente clasificado y fuera del margen,
	- $0 < \zeta^{(i)} < 1~$ si el ejemplo $~{\bf x}^{(i)}~$ está correctamente clasificado PERO dentro del margen,
	- $\zeta^{(i)} > 1~$ si el ejemplo $~{\bf x}^{(i)}~$ está mal clasificado.

- La restricción se convierte en:
$$
(y)\cdot(w_0 + w_1x_1 + w_2x_2) > 1-\zeta.
$$

- De esta manera $~\zeta~$ "afloja" la restricción cuando el resultado de $~(y)\cdot(w_0 + w_1x_1 + w_2x_2)~$ no llega a superar 1, bajando la restricción hasta $1-\zeta$.

- Hay una variable *slack* por cada ejemplo

*Algo <i>slack</i> es algo que no está apretado, que está aflojado*
*To slack = aflojar* 

Por último, esta expresión se puede generalizar para un modelo lineal de $D$ dimensiones simplemente escribiendo:
$$
y\cdot f({\bf x}, {\bf w}) > 1-\zeta.
$$

## Riesgo estructural
- Los parámetros óptimos $w^*$ serán aquellos que logren clasificar correctamente un ejemplos y, además, dejen el ejemplo fuera del margen.
- Para encontrarlos tendremos que plantear un problema de optimización.
- La diferencia con el problema planteado en los cuadernos pasados es que, ahora, dado un vector de parámetros $w$:
	- un ejemplo que quede bien clasificado y fuera del margen no contribuye a la función de pérdida (a diferencia de la regresión logística que bastaba con que el ejemplo quedase bien clasificado para que no contribuyese a la función de pérdida)
	- un ejemplo que quede bien clasificado, pero dentro del margen, contribuye con un valor de $0 < \zeta < 1$ 
	- un ejemplo que quede mal clasificado contribuye con un valor $\zeta > 1$ 

- Examinamos las restricciones:
Si despejamos $\zeta$ de la restricción que se obtiene de imponer el margen duro tenemos:

$$
\begin{align}
1-\zeta &< y \cdot f({\bf x},{\bf w})  \\
-\zeta  &< - 1 + y \cdot f({\bf x},{\bf w})  \\
\zeta   &> 1 - y \cdot f({\bf x},{\bf w})  \\
\end{align}
$$

La última desigualdad nos dice que:
- Si $\bf x$ está bien clasificado y además cae fuera del márgen entonces $y \cdot f({\bf x},{\bf w}) > 1$, por tanto al calcular $1 - y \cdot f({\bf x},{\bf w})$ obtenemos un número negativo.
	- Así que basta con que $\zeta = 0$ para ese ejemplo, y este ejemplo NO contribuye a la pérdida.
- Si $\bf x$ está bien clasificado pero cae dentro del margen entonces $0 < y \cdot f({\bf x},{\bf w}) < 1$, por tanto al calcular $1 - y \cdot f({\bf x},{\bf w})$ obtenemos un número positivo. 
	- Así que $~\zeta~$ tiene que ser _un infinitésimo_ mayor que ese número positivo para ese ejemplo.
- Si $\bf x$ está mal clasificado, entonces $0 < y \cdot f({\bf x},{\bf w})$ porque el valor target $y$ y la predicción del modelo $f({\bf x},{\bf w})$ tendrán signos opuestos, por tanto al calcular $1 - y \cdot f({\bf x},{\bf w})$ obtenemos un número positivo mayor que 1.
	- Así que $~\zeta~$ tiene que ser _un infinitésimo_ mayor que ese número positivo para ese ejemplo.


## Pérdida de Hinge
Sea:
$$

\mathcal{L} = \max\big( 0 ~,~ 1 - y \cdot f({\bf x},{\bf w}) \big).

$$
- Si ${\bf x}$ está bien clasificado entonces $~y \cdot f({\bf x},{\bf w}) < 0~$ y por tanto el máximo devuelve $0$. O sea que, efectivamente, ${\bf x}$ **no contribuye**.
- En cualquier otro caso $~y \cdot f({\bf x},{\bf w}) > 0~$ y por tanto el máximo NO devuelve 0. O sea que, efectivamente, **sí contribuye**.

- Así que este término buscará parámetros de tal manera que hay el menor número posible de ejemplos dentro del margen

- Además hemos visto que cuanto menor sea $||w||^2$ mayor será el margen.  Por lo que podemos juntarlo en un mismo término para crear la función de pérdida llamada *Hinge Loss*:
$$
\mathcal L_{\text Hinge} = \max\big(0,~~1-y\cdot f({\bf x},{\bf w})) + \frac{\alpha}{2}\Vert{\bf w}\Vert^2
$$
IMPORTANTE:
1. La pérdida _Hinge_ tiene dos términos:
  - $\max\big( 0 ~,~ 1 - y \cdot f({\bf x},{\bf w}) \big)$ es la contribución de cada ejemplo a la pérdida.
  - $\frac{1}{2}\Vert{\bf w}\Vert^2$ es un término que sólo depende de los parámetros, y que tiene la misma expresión que la regularización L2 o _Ridge_.
2. La expresión de arriba de $\mathcal{L}_{\rm Hinge}$ es para un único ejemplo $\bf x$. <br>
Para un conjunto de entrenamiento promediamos.<br>
En defintiva, la expresión final para el conjunto de entrenamiento con $N$ ejemplos es:

$$
\mathcal L_{\text Hinge} = \frac{1}{N}\sum\limits_{i=1}^N \left(
\max\big(0,~~1-y\cdot f({\bf x}^{(i)},{\bf w})\big)
+ \frac{\alpha}{2}\Vert{\bf w}\Vert^2 \right).
$$


Objetivo: $max(margen) == min(||w||^2 == w^Tw)$
Sabemos que: $\varepsilon = \frac{1}{||w||}$

![[1-(1).png]]

## Ejemplo de pérdida hinge con descenso de gradientes:
- Usamos el conjunto de datos iris para este ejemplo.
- Tenemos 3 clases, nos quedamos con q
- Con la clase `SGDClasifier` tenemos una aproximación a un descenso de gradiente (si lo queremos exacto lo tenemos que programar nosotros (ejercicio que mandó en otro notebook))
- El problema trata de separar los círculos de las estrellas, se puede hacer perfectamente de hecho queda con `accuracy = 1.0`.
- 