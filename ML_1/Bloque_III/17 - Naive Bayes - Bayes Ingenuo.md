
```insta-toc
---
title:
  name: ""
  level: 1
  center: false
exclude: ""
style:
  listType: dash
omit: []
levels:
  min: 1
  max: 6
---

# 

- Naive Bayes
    - ¬øQu√© es?
    - Ejemplos de iid
    - Caracter√≠sticas independientes
- Algoritmo para construir el modelo de verosimilitud NB
- Ejemplo de un NB-Bernoulli
- Ejemplo de una NB-Gaussiano
- Resumen
    - ¬øQu√© es?
    - Teorema de Bayes
    - ¬øC√≥mo funciona Na√Øve Bayes?
        - Entrenamiento
        - Predicci√≥n
    - Tipos de Na√Øve Bayes
        - Na√Øve Bayes Gaussiano
        - Na√Øve Bayes Multinomial
        - Na√Øve Bayes de Bernoulli
    - Ventajas de Na√Øve Bayes
    - Desventajas de Na√Øve Bayes
    - Ejemplos de Aplicaci√≥n
```


![[Implementing-Naive-Bayes-Classification-using-Python-1-1.webp]]
## Naive Bayes
### ¬øQu√© es?
- Se llama Naive o ingenuo porque sume que todas las caracter√≠sticas son independientes entre s√≠, lo cual no siempre es cierto en la vida real, pero funciona realmente bien en muchos casos
- Es el resultado de imponer dos condiciones a la hora de construir el modelo de verosimilitud: 
	- Todos los ejemplos provienen de la misma distribuci√≥n y son independientes entre s√≠. A esta condici√≥n la llamamos idd (independencias e identicamente distribuidos)
	- Las caracter√≠sticas son variables aleatorias independientes
### Ejemplos de iid
- La primera condici√≥n es muy **leve**
- Pr√°cticamente siempre que se trabaja con datos tabulados y sin orden se cumple

### Caracter√≠sticas independientes
- La segunda condici√≥n es **muy fuerte**
- Es muy frecuente que dos caracter√≠sticas tengan una relaci√≥n entre ellas de modo que al considerarlas independientes estamos simplificando su modelo.
- Cuando dos variables aleatorias $X$  e $Y$ se consideran independientes entonces su probabilidad conjunta es el producto de la probabilidad de cada una, es decir, $p(X,Y) = p(X)p(Y)$ 
- Si consideramos que cada caracter√≠stica es una variable aleatoria independiente, entonces el modelo de verosimilitud para los ejemplos de la clase $k$ es: $p({\bf X}|y=k) = \prod\nolimits_{j=1}^D p(x_j|y=k).$

Tomando logaritmos para evitar los problemas num√©ricos en el ordenador, tenemos
$$

\log p({\bf X}|y=k) = \sum\limits_{j=1}^D \log p(x_j|y=k)

$$
## Algoritmo para construir el modelo de verosimilitud NB
- Separar los ejemplos seg√∫n su clase
- Para cada clase diferente $k$:
	- Para cada caracter√≠stica de la tabla de ejemplos $x_j$:
		1. Elegir la distribuci√≥n de probabilidad $p(x_j|y=k; ~ \xi_{[j,k]}),~$ donde $\xi_{[j,k]}$ son los par√°metros de dicha distribuci√≥n.
		2. Estimar los par√°metros $\xi_{[j,k]}^*$ a partir de los valores de la caracter√≠stica $x_j$ para la clase $k$.
  
 - El modelo de verosimilitud de la clase $k$ es:
¬†$$
\log p(\mathbf{X} \mid y = k) = \sum_{j=1}^{D} \log p(x_j \mid y = k; ~ \xi_{[j,k]}^*).
$$

## Ejemplo de un NB-Bernoulli
Si una caracter√≠stica $j$ solo puede tomar dos valores, es decir $~x_j\in\{0,1\},~$ entonces su distribuci√≥n de probabilidad es Bernoulli, y por tanto
$$
\log p(x_j|y=k) =
\log \mathrm{Ber}\left( x_j |  y=k; ~\xi_{[j,k]} \right).
$$

Los par√°metros de una distribuci√≥n Bernoulli son el recuento de veces que aperece cada posibilidad.
![[Pasted image 20250402011933.png]]

## Ejemplo de una NB-Gaussiano
Si una caracter√≠stica $j$ tiene una distribuci√≥n normal $~x_j\sim\mathcal{N}(x_j; \mu, \sigma),~$ los par√°metros son la media y la desviaci√≥n.

![[Drawing 2025-04-02 01.21.41.excalidraw|1400]]

## Resumen 
### ¬øQu√© es?

Na√Øve Bayes es un **algoritmo de clasificaci√≥n basado en probabilidad** que utiliza el **Teorema de Bayes**. Se llama "Na√Øve" (ingenuo) porque **asume que todas las caracter√≠sticas son independientes** entre s√≠, lo cual no siempre es cierto, pero a√∫n as√≠ funciona bien en muchos casos.

### Teorema de Bayes

La base de este algoritmo es la siguiente f√≥rmula:

P(C‚à£X)=P(X‚à£C)P(C)P(X)P(C | X) = \frac{P(X | C) P(C)}{P(X)}

Donde:

- **P(C‚à£X)P(C | X)** ‚Üí Probabilidad de que XX pertenezca a la clase CC (**posterior**).
    
- **P(X‚à£C)P(X | C)** ‚Üí Probabilidad de observar XX si la clase es CC (**verosimilitud**).
    
- **P(C)P(C)** ‚Üí Probabilidad previa de la clase CC (**prior**).
    
- **P(X)P(X)** ‚Üí Probabilidad de observar XX (**evidencia**, constante para todas las clases).
    

El algoritmo **predice la clase con la mayor probabilidad posterior**.

---

### ¬øC√≥mo funciona Na√Øve Bayes?

El algoritmo sigue estos pasos:

#### Entrenamiento

- Se parte de un conjunto de datos etiquetado con varias clases.
    
- Se calculan **las probabilidades previas** de cada clase P(C)P(C).
    
- Se calculan **las probabilidades condicionales** de cada caracter√≠stica dado cada clase P(X‚à£C)P(X|C).
    

#### Predicci√≥n

Para clasificar un nuevo dato XX:

- Se multiplica la probabilidad condicional de cada caracter√≠stica.
    
- Se elige la clase CC con mayor P(C‚à£X)P(C | X).
    

üîπ **Ejemplo**: Clasificaci√≥n de correos como **spam** o **no spam**  
Supongamos que tenemos un correo con las palabras {gratis, oferta, descuento}.

- El modelo calcula la probabilidad de que el correo sea spam bas√°ndose en la frecuencia de esas palabras en correos spam vs. no spam.
    
- Se asigna la categor√≠a con la mayor probabilidad.
    

---

### Tipos de Na√Øve Bayes

#### Na√Øve Bayes Gaussiano

- Se usa para **datos num√©ricos continuos**.
    
- Se asume que los datos siguen una **distribuci√≥n normal (Gaussiana)**.
    
- **Ejemplo:** Clasificaci√≥n de im√°genes seg√∫n colores o caracter√≠sticas f√≠sicas.
    

#### Na√Øve Bayes Multinomial

- Se usa para **conteo de datos discretos**, como palabras en un documento.
    
- Se aplica mucho en **Procesamiento de Lenguaje Natural (NLP)**.
    
- **Ejemplo:** Clasificaci√≥n de textos (an√°lisis de sentimientos, detecci√≥n de spam).
    

#### Na√Øve Bayes de Bernoulli

- Se usa cuando los datos son **binarios (0 o 1)**.
    
- **Ejemplo:** Detecci√≥n de palabras clave en correos electr√≥nicos (si una palabra aparece o no).
    

---

### Ventajas de Na√Øve Bayes

‚úî **R√°pido y eficiente**: Funciona bien con grandes vol√∫menes de datos.  
‚úî **Escalable**: Puede procesar miles o millones de datos con rapidez.  
‚úî **Funciona bien con datos ruidosos o faltantes**.  
‚úî **Especialmente √∫til en clasificaci√≥n de texto y NLP**.

---

### Desventajas de Na√Øve Bayes

‚úñ **Suposici√≥n de independencia**: En la realidad, muchas caracter√≠sticas est√°n correlacionadas, lo que puede afectar la precisi√≥n.  
‚úñ **Problema de probabilidad cero**: Si un valor no aparece en los datos de entrenamiento, su probabilidad ser√° 0 y afectar√° el c√°lculo (se soluciona con **suavizado de Laplace**).  
‚úñ **No es bueno con datos num√©ricos sin distribuci√≥n normal** (salvo en su versi√≥n Gaussiana).

---

### Ejemplos de Aplicaci√≥n

üîπ **Filtrado de Spam** ‚Üí Identificar si un correo es spam o no.  
üîπ **Clasificaci√≥n de Sentimientos** ‚Üí Analizar si un comentario es positivo o negativo.  
üîπ **Diagn√≥stico M√©dico** ‚Üí Determinar enfermedades a partir de s√≠ntomas.  
üîπ **Reconocimiento de Texto** ‚Üí Clasificaci√≥n autom√°tica de documentos.
