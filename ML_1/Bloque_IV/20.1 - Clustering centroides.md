```insta-toc
---
title:
  name: ""
  level: 1
  center: false
exclude: ""
style:
  listType: dash
omit: []
levels:
  min: 1
  max: 6
---

# 

- Aprendizaje NO supervisado
- ¿Qué es un cluster?
- Definición de distancia
    - Distancia Manhattan y Euclidea
- Algoritmo k-medias
    - Pasos previos
    - Inicialización
    - Algoritmo
    - Variante k-medianas
    - Variante k-medoides
- Selección de k
    - Método del codo
    - Método silhouette
```
## Aprendizaje NO supervisado
- Hasta ahora cada ejemplo del conjunto de datos de entrenamiento $x$ venía emparejado con un valor target $y \in \mathbb{R}$  , o con una etiqueta $y$ perteneciente al conjunto de clases diferentes que hay.

- Se puede hacer aprendizaje automático si no tenemos $y \rightarrow SÍ \rightarrow$ aprendizaje no supervisado   

En concreto se pueden realizar las siguientes tareas (no necesariamente disjuntas):
- Visualización de los datos  (_Data visualization_)
	- En general los ejemplos _viven_ en un espacio $D$-dimensional, que NO podemos representar fácilmente en un papel o una pantalla salvo si $D\le 3$. 
	- Con ciertos "trucos" se podrían llegar a visualizar hasta $D=6$. 
	- La alternativa es reducir la dimensionalidad, tal y como hicimos con PCA. 
	- Otras técnicas más eficientes se agrupan bajo el título de **Aprendizaje de variedades** (_Manifold learning_).

- Detección de anomalías (_Anomaly detection_)
	- El objetivo es aprender a reconstruir los datos mediante aprendizaje supervisado, usando los propios datos como valor objetivo. 
	- Aunque se emplean técnicas supervisadas, al no haber etiquetas se considera NO-supervisado.

- Detección de novedades (_Novelty detection_)
	- El objetivo es aprender un modelo de los datos de entrenamiento que reconozca cuándo datos nuevos NO pertenecen al ese conjunto. También se le denomina  "clasificación de 1 sola clase" (_1-class classification_).

- Agrupamiento (_Clustering_)
	- Consiste en asignar cada ejemplo a un grupo en función de la proximidad o lejanía de los ejemplos entre sí.

En este cuaderno aprenderemos a realizar clustering basado en centroides.<br>
## ¿Qué es un cluster?
[[20.0 - Clustering y cluster]]
## Definición de distancia
e distancia
La **distancia** entre dos puntos $A$ y $B$, representado como $d(A,B)$, es toda expresión matemática que cumpla 4 condiciones muy intuitivas.
1. $d(A,B)  > 0$. 
   O sea  que la distancia no puede ser un número negativo.
2. $d(A,B) = 0$ si y solo si $A=B$.
    Es decir que la distancia entre dos puntos que están juntos es 0
3. $d(A,B) = d(B,A)$
    La distancia para ir de $A$ hasta $B$ es la misma que para volver de $B$ hacia $A$.
4. $d(A,B) + d(B,C) \le d(A,C)$
    O sea que el camino para ir desde $A$ hasta $C$ directamente es igual de largo o menor que si llego hasta $C$ por otro camino que pase por $B$.

![[ChatGPT Image 10 abr 2025, 12_11_10 1.png|375]]

### Distancia Manhattan y Euclidea
Dos distancias muy típicas son la Manhattan y la Euclidea.
En ambos casos asumimos que tenemos dos puntos en un espacio $D$ dimensional; es decir $A, B \in \mathbb R^{D}$ tal que $A = (a_1, a_2, \ldots, a_D)$ y $B = (b_1, b_2, \ldots, b_D)$

- La distancia Manhattan se define como:
$$
d(A, B) = \vert b_1 - a_1 \vert + \vert b_2 - a_2 \vert + \cdots + \vert b_D - a_D \vert
$$
- La distancia Euclidea es:
$$
d(A,B)=\sqrt{(b_1-a_1)^2 + (b_2-a_2)^2 + \cdots +(b_D-a_D)^2 }
$$

La distancia euclidea se puede generalizar para cualquier exponente $m$ haciendo:
$$
d(A,B)=\left((b_1-a_1)^m + (b_2-a_2)^m + \cdots +(b_D-a_D)^m \right)^{\frac{1}{m}}
$$
(conocida como distancia Minkowski)

En la figura de abajo se muestra una interpretación de las distancias Manhattan y Euclidea para $D=2$.
- A la izquierda se calcula $d(A,B)$ con la distancia Manhattan.
- A la derecha se calcula también con la distancia Euclidea (en verde).
![[Pasted image 20250410121445.png|375]]

## Algoritmo k-medias
### Pasos previos
- Debemos de elegir una métrica para la distancia
- Debemos elegir el número de clusters que queremos hacer (K)
- Llamaremos centroide al valor medio, es decir, calculando con la media de todos los ejemplos asignados a un mismo cluster.
	- Un centroide es un ejemplos que representa el centro de un cluster en torno al cual se agruparán los ejemplos que hayamos decidido asignar a dicho cluster
	- Por eso a veces también lo llamamos "prototipo de cluster"
	- Como cada ejemplo esta representado por un vector, el centroide es otro vector del mismo tamaño, pero dicho centroide de algún modo tiene que ser obligatoriamente uno de los ejemplos del conjunto de datos
### Inicialización
- Elegiremos aleatoriamente la posición de los K centroides
### Algoritmo
1. Medir la distancia del centroide a cada ejemplo del conjunto de datos
	1. Como resultado tendremos una lista de distancias por cada centroide, con tantos elementos como ejemplos haya en el conjunto de datos dado
2. Asignar cada ejemplo al cluster del centroide más cercano
	1. En este paso recorremos cada uno de los ejemplos mirando cual de los centroides está más proóximo. Como cada centroide es el prototipo de un cluster, el que quede más cerca del ejemplo mirado será al que asignemos dicho ejemplo.
3. Actualizar el centroide de cada cluster calculando la media de todos sus ejemplos
	1. Al terminar, calculamos el centroide de todos los ejemplos asignados a un mismo cluster, y repetimos esta operación para todos los centroides.
4. Volver al paso 1 y repetir cierto númeor de veces fijando al inicio o hasta que los centroides ya no cambien de posición.
	1. Es importante resltar que, al volver al paso 1 los ejemplos dejan de pertenecer al clueter que les habíamos asignado y se debe de repetir todo el proceso.
	2. Al repetir el paso 2 será cuando volvamos a asignar cada ejemplo a un cluster
![[Sin título 1.png]]
![[Sin título.gif]]
### Variante k-medianas
Se ha observado que k-medianas es mas robisto frente a ejemplos outliers
### Variante k-medoides
Recordamos que el prototipo no tiene porque ser uno de los ejemplos.
En este caso los centroides deben de ser ejemplos del conjunto de datos.
## Selección de k 
El número de clusters $K$ es selección del humano, por tanto nos interesa tener un método objetivo para decidirlo.
### Método del codo
El método del codo (elbow) consiste en representar en una gráfica una métrica que nos informe sobre lo buena o mala que es la selección de $K$.
En estas gráficas, a veces, se aprecia un "codo" en el valor de $K$ que deberíamos de elegir.
- Respecto a las posibles métricas tenemos: 
	- WCSS: (within cluster sum of squares) consiste en sumar la distnacia desde el centroide de un cluster a cada ejemplos asiganado a él y leugo sumar todos los resultados. Este calculo se hace para varios $K$ y después se represnta el WCSS frente a $K$ para visualizar el codo si es que se ve
	- ![[Sin título 1 1.png|371]]
	- Podemos apreciar un "codo" en $K=2$ porque la caída de WCSS cuando $K$ pasa de 1 a 2 es tan grande como la caida entre $2 \space y \space 10$ 
	- También podemos comprobar que los centroides están obtenidos cerca de los reales.
### Método silhouette
El sihoutte score es una medida de similitud de un ejemplos respecto de los otros ejemplos del mismo cluster al que han sido asignados y al mismo tiempo de la no-similitud con los ejemplos de otros clusters.
- Para cada ejemplo ${\bf x}^{(i)}$ asignado al cluster $k$ se calcula

$$

a(i) = \frac{1}{N_k -1}\sum\limits_{j=1}^{N_j} d({\bf x}^{(i)}, {\bf x}^{(j)}),

$$

donde ${\bf x}^{(j)}$ son ejemplos del cluster $k$.
- Esta expresión es una medida de lo no-similar que $x^{(i)}$ respecto a los otros ejmplos del cluster al que ha sido asignado
- Cuando menor sea el valor $a(i)$, más pequeñas serán las distancias calculadas, es decir, más similares serán los ejemplos que estamos comparando, y por lo tanto mejor será la asignación
- En estos casos se dice que el ejemplos i es similar al cluster k

- De un modo similar podemos definir la no-similitud de un ejemplo ${\bf x}^{(i)}$ con un cluster $\ell$ diferente al que ha sido asignado (que es $k$):

$$
b(i) = \mathop{\min}\limits_{\ell \neq k} \left( \frac{1}{N_\ell} \sum\limits_{j=1}^{N_\ell} d({\bf x}^{(i)}, {\bf x}^{(j)}) \right)
$$

- El resultado del paréntesis es un escalar, por lo que el proceso se debe repetir para cada cluster al que el ejemplo $x^{(i)}$ NO ha sido asignado.
- De esta manera obtenemos un vector de valores sobre el que se calcula el minimo.
- El cluster que da lugar el valor mínimo es el cluster vecino que es el siguiente mejor donde encajaria el ejmplo $x^{(i)}$
- Finalmente se define el _silhouette score_ para el ejemplo ${\bf x}^{(i)}$:

$$
s(i) =
\begin{cases}
1 - \frac{a(i)}{b(i)} & \text{si } a(i) < b(i) \\
0 & \text{si } a(i) = b(i) \\
\frac{b(i)}{a(i)} - 1 & \text{si } a(i) > b(i)
\end{cases}
$$


  

> $s(i)$ puede tomar cualquier valor en el intervalo $[-1,+1]$.
- $+1$ significa que ${\bf x}^{(i)}$ está muy alejado del cluster vecino.
- $0$ significa que ${\bf x}^{(i)}$ está muy cerca de la frontera entre dos clusters.
- $-1$ significa que ${\bf x}^{(i)}$ estaría mejor asignado a otro cluster.

![[Pasted image 20250519102442.png]]
![[Pasted image 20250519102454.png]]
![[Pasted image 20250519102507.png]]

